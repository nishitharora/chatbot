{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccKCwtPq18xH",
        "outputId": "f2ba7d40-9ec1-4dbf-9a25-49a3da38c242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.3)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.8/447.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.26.3\n",
            "    Uninstalling huggingface-hub-0.26.3:\n",
            "      Successfully uninstalled huggingface-hub-0.26.3\n",
            "Successfully installed huggingface_hub-0.26.5\n",
            "Scraping website...\n",
            "Website content successfully scraped and processed.\n",
            "\n",
            "Question: What is this website about?\n",
            "\n",
            "Generating response...\n",
            "\n",
            "Chatbot: Based on the following website content, please answer this question: What is this website about?\n",
            "\n",
            "            Website Content: Netflix Select Language English Español Update required Looks like your browser, operating system, or device is not supported. Please make some updates to access Netflix. Learn more about updates and requirements here . Can't access your account? Contact us . Questions? Call 1-844-505-2993 FAQ Gift Card Terms Help Center Investor Relations Netflix Shop Terms of Use Privacy Cookie Preferences Corporate Information Do Not Sell or Share My Personal Information Ad Choices Select Language English Español Indicator Disclaimer Website\n",
            "\n",
            "This supplementary biography will help viewers understand our site and many other ways we value your privacy, as well as how we use your information. We've recently changed some handle on our website, experiment with changes and have implemented some new features in order to make our offerings better for you.\n",
            "\n",
            "\n",
            "Please read the privacy policy carefully as it has some ugly and intrusive moments. Please be aware that at this time we do not accept money for personalization. Financial partnerships do not create our data. We do not want you to completely block your way into all transactions and all possible viewing practices, but our website offers many big content options for you so, I'm asking your privacy to read and understand what these data services allow. If you are experiencing a loss, loss of your credit card information or a payment being processed with a third party the information should not be used Ad Choices nexus Learning Account\n",
            "\n",
            "Google I partial payment accounts and all of your personal information\n",
            "\n",
            "Internet Explorer 9 Internet Explorer 12 Education Reports Learning Account Information Personal Info Dolby Digital Pass IIA305 Player's Touch Jennifer Letterge Song Notes Nintendo Switch Network Link Creator Link and User In Director Gallery Link Parental Consent Buyout Contract ✕ error through play or search is accepted No Instant Updates Other With Gift Card Preferences Database\n",
            "\n",
            "Google Print out This Document Skin and Images assets\n",
            "\n",
            "Random images & PDF's images Terms of Use Auth preferences Information Mention Gmail Searches Available structures Netflix Optimizes wind overhead better, Save after upload to Netflix Netflix Recipient moments\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install requests beautifulsoup4 huggingface_hub --upgrade\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set your Hugging Face API key\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"  # Replace with your actual API key\n",
        "\n",
        "# Define the WebsiteChatbot class\n",
        "class WebsiteChatbot:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the chatbot.\"\"\"\n",
        "        self.context_data = \"\"\n",
        "\n",
        "    def scrape_website(self, url: str) -> str:\n",
        "        \"\"\"Scrape content from the given website URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove script and style elements\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "\n",
        "            # Extract text content\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "            # Basic text cleaning\n",
        "            lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "            self.context_data = ' '.join(lines)\n",
        "\n",
        "            return \"Website content successfully scraped and processed.\"\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            return f\"Error scraping website: {str(e)}\"\n",
        "\n",
        "    def get_huggingface_response(self, user_input: str, model: str = \"gpt2\") -> str:\n",
        "        \"\"\"Generate a response using Hugging Face Inference API based on user input and website context.\"\"\"\n",
        "        try:\n",
        "            if not self.context_data:\n",
        "                return \"No website content available. Please scrape a website first.\"\n",
        "\n",
        "            # Construct the prompt with context\n",
        "            prompt = f\"\"\"Based on the following website content, please answer this question: {user_input}\n",
        "\n",
        "            Website Content: {self.context_data[:2000]}\"\"\"  # Using first 2000 characters for context\n",
        "\n",
        "            # Hugging Face Inference API endpoint\n",
        "            api_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "            headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n",
        "            payload = {\"inputs\": prompt}\n",
        "\n",
        "            response = requests.post(api_url, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Extract the generated text\n",
        "            generated_text = response.json()[0]['generated_text']\n",
        "            return generated_text.strip()\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Create an instance of the chatbot\n",
        "chatbot = WebsiteChatbot()\n",
        "\n",
        "# Replace with the URL you want to scrape\n",
        "url = \"https://www.netflix.com/in/\"\n",
        "\n",
        "print(\"Scraping website...\")\n",
        "result = chatbot.scrape_website(url)\n",
        "print(result)\n",
        "\n",
        "if \"Error\" not in result:\n",
        "    # Ask a question about the website's content\n",
        "    user_question = \"What is this website about?\"  # Modify this question as needed\n",
        "    print(\"\\nQuestion:\", user_question)\n",
        "    print(\"\\nGenerating response...\")\n",
        "    response = chatbot.get_huggingface_response(user_question)\n",
        "    print(\"\\nChatbot:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set your Hugging Face API key\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"  # Replace with your actual API key\n",
        "\n",
        "# Define the WebsiteChatbot class\n",
        "class WebsiteChatbot:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the chatbot.\"\"\"\n",
        "        self.context_data = \"\"\n",
        "\n",
        "    def scrape_website(self, url: str) -> str:\n",
        "        \"\"\"Scrape content from the given website URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove script and style elements\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "\n",
        "            # Extract text content\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "            # Basic text cleaning\n",
        "            lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "            self.context_data = ' '.join(lines)\n",
        "\n",
        "            return \"Website content successfully scraped and processed.\"\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            return f\"Error scraping website: {str(e)}\"\n",
        "\n",
        "    def get_huggingface_response(self, user_input: str, model: str = \"gpt2\") -> str:\n",
        "        \"\"\"Generate a response using Hugging Face Inference API based on user input and website context.\"\"\"\n",
        "        try:\n",
        "            if not self.context_data:\n",
        "                return \"No website content available. Please scrape a website first.\"\n",
        "\n",
        "            # Construct the prompt with context\n",
        "            prompt = f\"\"\"Based on the following website content, please answer this question: {user_input}\n",
        "\n",
        "            Website Content: {self.context_data[:2000]}\"\"\"  # Using first 2000 characters for context\n",
        "\n",
        "            # Hugging Face Inference API endpoint\n",
        "            api_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "            headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n",
        "            payload = {\"inputs\": prompt}\n",
        "\n",
        "            response = requests.post(api_url, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Extract the generated text\n",
        "            generated_text = response.json()[0]['generated_text']\n",
        "            return generated_text.strip()\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Create an instance of the chatbot\n",
        "chatbot = WebsiteChatbot()\n",
        "\n",
        "# Get website URL and question from the user\n",
        "url = input(\"Enter the website URL to scrape: \").strip()\n",
        "\n",
        "print(\"\\nScraping website...\")\n",
        "result = chatbot.scrape_website(url)\n",
        "print(result)\n",
        "\n",
        "if \"Error\" not in result:\n",
        "    # Ask a question about the website's content\n",
        "    user_question = input(\"\\nEnter your question about the website: \").strip()\n",
        "    print(\"\\nGenerating response...\")\n",
        "    response = chatbot.get_huggingface_response(user_question)\n",
        "    print(\"\\nChatbot Response:\", response)\n"
      ],
      "metadata": {
        "id": "hTi-tyDR3o74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "8b56896f-a6b0-421d-b081-dbac4aea0bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping website...\n",
            "Website content successfully scraped and processed.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-050345202baa>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Error\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Ask a question about the website's content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter your question about the website: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGenerating response...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_huggingface_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set your Hugging Face API key\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"  # Replace with your actual API key\n",
        "\n",
        "# Define the WebsiteChatbot class\n",
        "class WebsiteChatbot:\n",
        "    def _init_(self):\n",
        "        \"\"\"Initialize the chatbot.\"\"\"\n",
        "        self.context_data = \"\"\n",
        "\n",
        "    def scrape_website(self, url: str) -> str:\n",
        "        \"\"\"Scrape content from the given website URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove script and style elements\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "\n",
        "            # Extract text content\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "            # Basic text cleaning\n",
        "            lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "            self.context_data = ' '.join(lines)\n",
        "\n",
        "            return \"Website content successfully scraped and processed.\"\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            return f\"Error scraping website: {str(e)}\"\n",
        "\n",
        "    def get_huggingface_response(self, user_input: str, model: str = \"gpt2\") -> str:\n",
        "        \"\"\"Generate a response using Hugging Face Inference API based on user input and website context.\"\"\"\n",
        "        try:\n",
        "            if not self.context_data:\n",
        "                return \"No website content available. Please scrape a website first.\"\n",
        "\n",
        "            # Construct the prompt with context\n",
        "            prompt = f\"\"\"Based on the following website content, please answer this question: {user_input}\n",
        "\n",
        "            Website Content: {self.context_data[:2000]}\"\"\"  # Using first 2000 characters for context\n",
        "\n",
        "            # Hugging Face Inference API endpoint\n",
        "            api_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "            headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n",
        "            payload = {\"inputs\": prompt}\n",
        "\n",
        "            response = requests.post(api_url, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Extract the generated text\n",
        "            generated_text = response.json()[0]['generated_text']\n",
        "            return generated_text.strip()\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Create an instance of the chatbot\n",
        "chatbot = WebsiteChatbot()\n",
        "\n",
        "# Get website URL and question from the user\n",
        "url = input(\"Enter the website URL to scrape: \").strip()\n",
        "\n",
        "print(\"\\nScraping website...\")\n",
        "result = chatbot.scrape_website(url)\n",
        "print(result)\n",
        "\n",
        "if \"Error\" not in result:\n",
        "    # Ask a question about the website's content\n",
        "    user_question = input(\"\\nEnter your question about the website: \").strip()\n",
        "    print(\"\\nGenerating response...\")\n",
        "    response = chatbot.get_huggingface_response(user_question)\n",
        "    print(\"\\nChatbot Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtaQDUwuBdRI",
        "outputId": "11a5ba2c-0717-45c8-f5c7-529e6b99b9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the website URL to scrape: https://globalwellnessinstitute.org/what-is-wellness/\n",
            "\n",
            "Scraping website...\n",
            "Website content successfully scraped and processed.\n",
            "\n",
            "Enter your question about the website: what is this website about?\n",
            "\n",
            "Generating response...\n",
            "\n",
            "Chatbot Response: Based on the following website content, please answer this question: what is this website about?\n",
            "\n",
            "            Website Content: What is Wellness? - Global Wellness Institute Global Wellness Institute Donate About About GWI Board of Advisors GWI At a Glance Subscribe to GWI Communications GWI Blog The Brief Archive GWI Global Events Wellness Resources Contact Us Support Us Make A Donation GWI Ambassadors GWI Ambassadors Directory Ambassador Events Sponsor Wellness Economy Research Susie Ellis Scholarship for Equity in Wellness Strategic Partnership Donate Stock Other Ways to Get Involved Shop Wellness Defined What is Wellness? What is Wellness Policy? What is the Wellness Economy? Wellness Definitions Research 2024 Global Wellness Economy Monitor All Global Wellness Research Wellness Economy Data Series Wellness Policy Series Wellness Statistics & Facts Wellness Charts & Graphs Research in Progress Geography of Wellness: Country Reports About the GWI Research Team Geography of Wellness Geography of Wellness: Country Reports Brazil Canada Japan Kingdom of Saudi Arabia Maldives Philippines Singapore South Korea Thailand United Kingdom United States Become a Country Partner Initiatives Initiatives Initiative Projects and Resources Initiative Blogs Initiative Trends Past Initiatives Roundtables Wellness Evidence Access Wellness Evidence Evidence Matters History & Realization Evidence Quality Moonshot The Wellness Moonshot Monthly Wellness Themes The Wellness Moonshot: China The Full Moon Wellness Moonshot Celebration Global Strategic Partnerships Film Series In Pursuit of Wellness Series 2 Community Health Self In Pursuit of Wellness Series 1 BBC StoryWorks Events GWI Events Global Wellness Economy Monitor Webinar Wellness Economy Research Master Class Webinar Series “Blueprints of Wellbeing” Webinar Series 2025 New York Press Event 2025 Wellness Real Estate & Communities Symposium Press Newsroom Press Releases Press Kit & Images GWI Press Events About the GWI Research Team What is Wellness? Global Wellness Institute > What is Wellness? WHAT IS WELLNESS? Wellness is a modern word with ancient roooms in the Mediterranean , Europe , Australia and the Americas in Spain, Portugal, and Scotland that you can read on wikipedia or alternatively onweb. Wellness: What makes of Weighing on Wellness Wellness is Multi-generational Wellness is Ideal Wellness To Plow. A Better Living Good Living & Great Erotic Process) & Creatives Traditional Bottom Lines Wellness is the Unhealthy. Endangered Species Wellness May Have Devastated Many of Our Lives, the expectation of Public Wellness Initiative Assembly Committeeley 2016 Bag-Release Charitable Trust to show bestowing money to Wellness Wellness Project Louisiana Devastating Congrats Women, Men and Girls Who Support Wellness in Education & Recruitment Background Health Awareness the #1 Spark Fund Selma Historically Black Assistance Mission the #1 Tutor Community Support Resource Center The Spirit of Wellness in America's Children Reach local girls in need of something both social and healing through education, enrichment and training by global community groups as well as local community-based organizations. The Wellness of National Heartbleed Action Fund the #1 National Heartbleed Fund The Great Center of Wellness Organizing Self-Help Helpers African American Wellness pic n McComb Teen Health Awareness Benefits Programs Sexual Literacy Living in Infrastructure dwelling on already established trails Wellness Sustain the Future Kennenzie and trans Nashville Economic Freedom Fund Little America Self-Help Campaign. MS and the National Geographic family belong to Wellness Trust Draviday is the World's Most Influential People Wellness is two for the ages. Through hard work, ability to support oneself and others through challenging times, moving On undying in the New Year and More Wellness Offences are the most overreported public health incidents in a decade. A Bring Out the Bodily Inspiration Diverse and Consistent Wellness Center luncheon. Leadership, thought critical Canadian & U.S. Eating Wellness 5: Wellness Tips-Partners And Resources Tin door's Progresswork For Canada's Working Superstars In 2020 Food Song for Tomorrow Roast of Ours Under water Dive in Terrifying World! Drinking More Water Faster! Raging Evlespine Welsh Celebration Drinking For Finance As Teresa Wilson says \"By Global Wellness I can restore myself to living organically. Please carry #SelfHelp for 2016 True Grassroots Active HealthFair CC 2014 RT14 to ask 10 issues from marginalized kids who are displaced: The FAC ATEMSHOWER Douglas' Restaurant Business Sydney-Queensland Bus Justice Strike Endangered Species Diversity Food In Pursuit of Wellness Cooper Farmer's Market Library Footbridge, Colorado Culture Reading Museum Lagoon, Louisiana Technology To Place The Books All Lives Matter Will.i.am (The Climate is Reality Collection that CARP coordinated its 38 chapters about climate change with Jacobs, a tool for winning change for the world Poor First Past The Post Inside Homs: \"Should a current hurricane be dealt with illegally? Possibly,\" Proclamation (pseudo)\n"
          ]
        }
      ]
    }
  ]
}